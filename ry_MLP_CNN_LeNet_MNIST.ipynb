{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMzZanYtX8wwEnmfgx+IFV8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renyuanL/_JosephLin_2023/blob/main/ry_MLP_CNN_LeNet_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Lu59i6vAleU"
      },
      "outputs": [],
      "source": [
        "# ry Implementing \n",
        "# a Multilayer Perceptron (MLP) \n",
        "# and a Convolutional Neural Network (CNN) \n",
        "# for MNIST classification\n",
        "# for tutorial purpose\n",
        "# 2023/04/22\n",
        "\n",
        "# https://paperswithcode.com/method/lenet\n",
        "\n",
        "\n",
        "\n",
        "# %%  References\n",
        "# https://github.com/patrickloeber/pytorchTutorial/blob/master/13_feedforward.py\n",
        "\n",
        "import torch\n",
        "import torch.nn             as nn\n",
        "import torch.nn.functional  as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms   as transforms\n",
        "\n",
        "import matplotlib.pyplot        as plt\n",
        "\n",
        "# Device configuration\n",
        "device= torch.device(\n",
        "    'cuda' if torch.cuda.is_available() else \n",
        "    'cpu')\n",
        "\n",
        "print(f'{device= }')\n",
        "\n",
        "##%% View data\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset= torchvision.datasets.MNIST(\n",
        "    root=       './data', \n",
        "    train=      True, \n",
        "    transform=  transforms.ToTensor(),  \n",
        "    download=   True)\n",
        "\n",
        "test_dataset= torchvision.datasets.MNIST(\n",
        "    root=       './data', \n",
        "    train=      False, \n",
        "    transform=  transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "\n",
        "# batch size, not too large due to memory constraint\n",
        "# not too small due to speed consideration\n",
        "# batch_size=   100  samples per batch\n",
        "# 60_000/100= 600 batches in train_loader\n",
        "# 10_000/100= 100 batches in test_loader\n",
        "\n",
        "batch_size=     100 \n",
        "\n",
        "train_loader= torch.utils.data.DataLoader(\n",
        "    dataset=    train_dataset, \n",
        "    batch_size= batch_size, \n",
        "    shuffle=    True)\n",
        "\n",
        "test_loader= torch.utils.data.DataLoader(\n",
        "    dataset=    test_dataset, \n",
        "    batch_size= batch_size, \n",
        "    shuffle= False)\n",
        "\n",
        "# %% take 1 batch of data\n",
        "\n",
        "examples= iter(test_loader)\n",
        "example_data, example_targets= next(examples)\n",
        "\n",
        "# view the images in the batch\n",
        "\n",
        "for i in range(100):\n",
        "    plt.subplot(10, 10, i+1)\n",
        "    if i<10:\n",
        "        plt.title(example_targets[i].item())\n",
        "    # not show axes\n",
        "    plt.axis('off')\n",
        "    plt.imshow(example_data[i][0])\n",
        "plt.show()\n",
        "\n",
        "for i in range(100):\n",
        "    lab= example_targets[i].item()\n",
        "    print(lab, end=' ')\n",
        "    if (i+1)%10==0:\n",
        "        print('')\n",
        "\n",
        "#%% Mlp class\n",
        "\n",
        "# Hyper-parameters \n",
        "h0= input_size=  28*28 # 784\n",
        "h1= h0//4              # 196\n",
        "h2= h1//4              # 49\n",
        "h3= num_classes= 10\n",
        "\n",
        "num_epochs=      10\n",
        "learning_rate= 0.01\n",
        "\n",
        "# Fully connected neural network with 2 hidden layers\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_size=  28*28, \n",
        "                 h1=          28*28//4,\n",
        "                 h2=          28*28//4//4, \n",
        "                 num_classes= 10):\n",
        "        \n",
        "        super(Mlp, self).__init__()\n",
        "        \n",
        "        self.f= nn.ReLU()\n",
        "        self.g= nn.Softmax(dim= -1)\n",
        "\n",
        "        self.l1= nn.Linear(input_size, \n",
        "                           h1) \n",
        "        self.l2= nn.Linear(h1,         \n",
        "                           h2)\n",
        "        self.l3= nn.Linear(h2,         \n",
        "                           num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "\n",
        "        y= self.l1(x);  y= self.f(y)\n",
        "        y= self.l2(y);  y= self.f(y)\n",
        "        y= self.l3(y);  y= self.g(y)  # softmax at the end\n",
        "        return y\n",
        "    \n",
        "    def predict(self, x):\n",
        "        \n",
        "        y= self.forward(x)\n",
        "        y_pred= torch.argmax(y, axis= -1)\n",
        "\n",
        "        return y_pred\n",
        "    \n",
        "    def train(self, train_loader, num_epochs= 10, learning_rate= .01):\n",
        "\n",
        "        loss=  nn.CrossEntropyLoss()\n",
        "        optim= torch.optim.Adam(\n",
        "            self.parameters(), \n",
        "            lr= learning_rate)  \n",
        "\n",
        "        # Train the model\n",
        "        num_batches= len(train_loader)\n",
        "        for epoch in range(num_epochs):\n",
        "            b= 0\n",
        "            for x, y_tgt in train_loader:\n",
        "                \n",
        "                # origin shape: [100, 1, 28, 28]\n",
        "                # reshape:      [100, 784]  \n",
        "                x=     x.reshape(    -1, 1*28*28).to(device)\n",
        "        \n",
        "                # Forward pass\n",
        "                y= self.forward(x)\n",
        "                \n",
        "                # compute loss\n",
        "                y_tgt= y_tgt.to(device)\n",
        "                e= loss(y, y_tgt)\n",
        "                \n",
        "                # Backward and optimize\n",
        "                optim.zero_grad()\n",
        "                e.backward()\n",
        "                optim.step()\n",
        "                b += 1\n",
        "                # show progress\n",
        "                if b % 100 == 0:\n",
        "                    print (f'{epoch}/{num_epochs}, {b}/{num_batches}, e: {e.item():.4f}')\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        # Test the model\n",
        "        # In test phase, we don't need to compute gradients \n",
        "        # (for memory efficiency)\n",
        "        with torch.no_grad():\n",
        "            n_correct= 0\n",
        "            n_samples= 0\n",
        "            for x, y_tgt in test_loader:\n",
        "                x=      x.reshape(-1, 1*28*28).to(device)\n",
        "                y_pred= self.predict(x)\n",
        "                \n",
        "                y_tgt= y_tgt.to(device)\n",
        "                n_correct += (y_pred== y_tgt).sum().item()\n",
        "                n_samples += len(y_tgt)\n",
        "            \n",
        "            acc= n_correct / n_samples\n",
        "            print(f'{acc= :.4f}')\n",
        "            \n",
        "        return {'acc': acc, 'n_correct': n_correct, 'n_samples': n_samples}\n",
        "\n",
        "mlp= Mlp(input_size, h1, h2, num_classes).to(device)\n",
        "mlp.train(train_loader)\n",
        "mlp.test(test_loader)\n",
        "\n",
        "# %% LeNet class\n",
        "\n",
        "class Cnn(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Cnn, self).__init__()\n",
        "        \n",
        "        self.convolutional_layer= nn.Sequential(\n",
        "            nn.Conv2d(in_channels=  1, \n",
        "                      out_channels= 16, \n",
        "                      kernel_size= (3,3), \n",
        "                      padding= 'same'),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(in_channels=  16, \n",
        "                      out_channels= 16, \n",
        "                      kernel_size=(3, 3),\n",
        "                      padding= 'same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size= (2,2), \n",
        "                         stride=      (2,2)),\n",
        "\n",
        "            nn.Conv2d(in_channels=  16, \n",
        "                      out_channels= 16, \n",
        "                      kernel_size= (3,3), \n",
        "                      padding= 'same'),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size= (2,2), \n",
        "                         stride=      (2,2)),\n",
        "        )\n",
        "\n",
        "        height= 28//2//2 # 7\n",
        "        width=  28//2//2 # 7\n",
        "\n",
        "        flatten_size= 16*height*width # 784\n",
        "\n",
        "        h1= flatten_size//4 # 196\n",
        "        h2= h1//4 # 49\n",
        "        \n",
        "\n",
        "        self.linear_layer= nn.Sequential(\n",
        "            nn.Linear(in_features= flatten_size, \n",
        "                      out_features= h1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features= h1, \n",
        "                      out_features=h2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features= h2, \n",
        "                      out_features=10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.convolutional_layer(x)\n",
        "        \n",
        "        x= torch.flatten(x, 1)\n",
        "\n",
        "        x= self.linear_layer(x)\n",
        "        y= F.softmax(x, dim=-1)\n",
        "        return y\n",
        "    \n",
        "    def predict(self, x):\n",
        "        \n",
        "        y= self.forward(x)\n",
        "        y_pred= torch.argmax(y, axis= -1)\n",
        "\n",
        "        return y_pred\n",
        "    \n",
        "    def train(self, train_loader, num_epochs= 10, learning_rate= .001):\n",
        "            \n",
        "        optimizer= torch.optim.Adam(self.parameters(), lr= learning_rate)\n",
        "        criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "        epochs= num_epochs\n",
        "        train_loss, val_loss = [], []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "        \n",
        "            total_train_loss = 0\n",
        "            total_val_loss = 0\n",
        "\n",
        "            # self.train()\n",
        "            #### super(LeNet5, self).train() # same as self.train() \n",
        "            # \n",
        "            # and self.eval() will turn on and off the training mode \n",
        "            # of our model.\n",
        "            \n",
        "            # training our model\n",
        "            for idx, (image, label) in enumerate(train_loader):\n",
        "\n",
        "                image, label = image.to(device), label.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                pred = self.forward(image)\n",
        "\n",
        "                loss = criterion(pred, label)\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_train_loss = total_train_loss / (idx + 1)\n",
        "            train_loss.append(total_train_loss)\n",
        "            \n",
        "            # validating our model\n",
        "            #### model.eval()\n",
        "\n",
        "            total = 0\n",
        "            for idx, (image, label) in enumerate(test_loader):\n",
        "                image, label = image.to(device), label.to(device)\n",
        "                \n",
        "                pred = self.forward(image)\n",
        "\n",
        "                loss = criterion(pred, label)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                pred = torch.nn.functional.softmax(pred, dim=1)\n",
        "                for i, p in enumerate(pred):\n",
        "                    if label[i] == torch.max(p.data, 0)[1]:\n",
        "                        total = total + 1\n",
        "\n",
        "            accuracy = total / len(test_loader.dataset)\n",
        "\n",
        "            total_val_loss = total_val_loss / (idx + 1)\n",
        "            val_loss.append(total_val_loss)\n",
        "\n",
        "            if epoch % 5 == 0:\n",
        "                print('\\nEpoch: {}/{}, Train Loss: {:.4f}, Val Loss: {:.4f}, Val Acc: {:.4f}'\n",
        "                        .format(epoch, epochs, total_train_loss, total_val_loss, accuracy))\n",
        "        plt.plot(train_loss)\n",
        "        plt.plot(val_loss)\n",
        "\n",
        "        return train_loss, val_loss\n",
        "    \n",
        "    def test(self, test_loader):\n",
        "\n",
        "        model= self\n",
        "\n",
        "        testiter = iter(test_loader)\n",
        "        images, labels = next(testiter)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            preds = model(images)\n",
        "\n",
        "        images_np = [i.mean(dim=0).cpu().numpy() for i in images]\n",
        "\n",
        "        class_names= ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "\n",
        "        fig = plt.figure(figsize=(20,20))\n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.02, wspace=0.02)\n",
        "\n",
        "        for i in range(100):\n",
        "            ax = fig.add_subplot(10, 10, i + 1, xticks=[], yticks=[])\n",
        "            ax.imshow(images_np[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "\n",
        "            if labels[i] == torch.max(preds[i], 0)[1]:\n",
        "                ax.text(0, 5, class_names[torch.max(preds[i], 0)[1]], \n",
        "                        color='green', fontsize=20)\n",
        "                # set the text font larger\n",
        "\n",
        "            else:\n",
        "                ax.text(0, 5, class_names[torch.max(preds[i], 0)[1]], \n",
        "                        color='red',  fontsize=30, fontweight='bold')\n",
        "                ax.text(20, 5, class_names[labels[i]], \n",
        "                        color='blue',  fontsize=30, fontweight='bold')\n",
        "# %% Train the model\n",
        "\n",
        "cnn= Cnn().to(device)\n",
        "cnn.train(train_loader)\n",
        "cnn.test(test_loader)\n",
        "\n",
        "#%%\n",
        "# %%\n",
        "class LeNet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    \n",
        "    super(LeNet, self).__init__()\n",
        "\n",
        "    self.conv1= nn.Conv2d(   1, 6, 5) \n",
        "    self.pool1= nn.MaxPool2d(2, 2) \n",
        "    self.conv2= nn.Conv2d(   6,16, 5) \n",
        "    self.pool2= nn.MaxPool2d(2, 2) \n",
        "    \n",
        "    self.fc1= nn.Linear(     16*4*4, 120) \n",
        "    # 16*4*4= 256\n",
        "    # why 4*4? \n",
        "    # this is the output size of the conv2 layer\n",
        "    # 28-5+1= 24, 24/2= 12, 12-5+1= 8, 8/2= 4 \n",
        "    \n",
        "    self.fc2= nn.Linear(     120,84)\n",
        "    self.fc3= nn.Linear(     84, 10) \n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    \n",
        "    x= self.pool1(F.relu(self.conv1(x))) \n",
        "    x= self.pool2(F.relu(self.conv2(x))) \n",
        "    \n",
        "    x= torch.flatten(x,1) \n",
        "    # flatten all dimensions except batch\n",
        "    # x= x.view(-1, 16*4*4) # same as above\n",
        "    # x= x.reshape(-1, 16*4*4) # same as above\n",
        "    # x.shape= (64, 256) # 64= batch_size, 256= 16*4*4\n",
        "\n",
        "    x= F.relu(self.fc1(x))\n",
        "    x= F.relu(self.fc2(x)) \n",
        "    logits= self.fc3(x)\n",
        "\n",
        "    # logits= output of the last layer \n",
        "    # logits.shape= (64, 10) # 64= batch_size, 10= number of classes\n",
        "    # logits are the raw values before the softmax function\n",
        "    # y= F.softmax(logits, dim=1)\n",
        "    # y will be the probability distribution over each class\n",
        "\n",
        "    return logits\n",
        "\n",
        "leNet= LeNet() \n",
        "print(leNet)\n",
        "\n",
        "\n",
        "opt=  optimizer= torch.optim.Adam(leNet.parameters(), lr= learning_rate)\n",
        "loss= criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def train(dataloader, model, loss, opt, \n",
        "          device= torch.device('cpu')):\n",
        "  \n",
        "  model= model.to(device)\n",
        "\n",
        "  size= len(train_loader.dataset)\n",
        "  for b, (x, y_tgt) in enumerate(dataloader):\n",
        "    x, y_tgt= x.to(device), y_tgt.to(device) \n",
        "    y_pred= model(x) \n",
        "    l=      loss(y_pred, y_tgt) \n",
        "\n",
        "    opt.zero_grad()\n",
        "    l.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if b % 100 ==0:\n",
        "      l, current = l.item(), b * len(x) \n",
        "      perplexity= np.exp(l)\n",
        "      print(f\"loss= {l:.6f},  perplexity= {perplexity:.6f}, [{current} / {size}]\")\n",
        "\n",
        "def test(dataloader, model, loss, \n",
        "         device= torch.device('cpu')):\n",
        "\n",
        "  model= model.to(device)\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader) \n",
        "  correct, test_loss = 0,0 \n",
        "  with torch.no_grad():\n",
        "    for x,y in dataloader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      y_pred = model(x) \n",
        "      test_loss += loss(y_pred, y).item() \n",
        "      correct += (y_pred.argmax(1) == y).type(torch.float).sum().item() \n",
        "  test_loss /= num_batches \n",
        "  correct /= size \n",
        "  print(f\"accuracy is {correct:.4f}, test_loss= {test_loss:.6f}, test_perplexity= {np.exp(test_loss):.6f}\")\n",
        "\n",
        "\n",
        "'''\n",
        "device= torch.device(\n",
        "  \"cuda\" if torch.cuda.is_available() else \n",
        "  \"cpu\")\n",
        "'''\n",
        "#%%\n",
        "print(f'{device= }')\n",
        "\n",
        "EPOCHS= num_epochs #= 10 \n",
        "\n",
        "for epochs in range(EPOCHS):\n",
        "  print(f\"epoch: {epochs+1} ---------------------------\")\n",
        "  train(train_loader, leNet, loss, opt, device)\n",
        "  test(test_loader, leNet, loss, device) \n",
        "\n",
        "print('DONE') \n",
        "# %%\n",
        "# the parameters of the model\n",
        "for name, param in leNet.named_parameters():\n",
        "    print(f'{name= }, {param.shape= }')\n",
        "\n",
        "# %%\n",
        "# the total number of parameters\n",
        "total_number_of_parameters= sum(\n",
        "    p.numel() for p in leNet.parameters())\n",
        "\n",
        "print(f'{total_number_of_parameters= }')\n",
        "# %%\n",
        "'''\n",
        "name= 'conv1.weight', param.shape= torch.Size([6, 1, 5, 5])\n",
        "name= 'conv1.bias', param.shape= torch.Size([6])\n",
        "name= 'conv2.weight', param.shape= torch.Size([16, 6, 5, 5])\n",
        "name= 'conv2.bias', param.shape= torch.Size([16])\n",
        "name= 'fc1.weight', param.shape= torch.Size([120, 256])\n",
        "name= 'fc1.bias', param.shape= torch.Size([120])\n",
        "name= 'fc2.weight', param.shape= torch.Size([84, 120])\n",
        "name= 'fc2.bias', param.shape= torch.Size([84])\n",
        "name= 'fc3.weight', param.shape= torch.Size([10, 84])\n",
        "name= 'fc3.bias', param.shape= torch.Size([10])\n",
        "total_number_of_parameters= 44_426\n",
        "'''\n",
        "qq= (\n",
        "      6*( 1*5*5+1) +\n",
        "     16*( 6*5*5+1) + \n",
        "    120*(16*4*4+1) + \n",
        "     84*(   120+1) + \n",
        "     10*(    84+1)\n",
        "    ) \n",
        "print(f'{qq= }')\n",
        "# %%\n"
      ]
    }
  ]
}