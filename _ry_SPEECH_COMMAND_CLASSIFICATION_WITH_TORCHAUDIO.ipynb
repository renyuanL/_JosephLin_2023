{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOxrTom+rxcPz4FJb/5FVgS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renyuanL/_JosephLin_2023/blob/main/_ry_SPEECH_COMMAND_CLASSIFICATION_WITH_TORCHAUDIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bQs2JSBIZHR"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "SPEECH COMMAND CLASSIFICATION WITH TORCHAUDIO\n",
        "\n",
        "ry modify from:\n",
        "https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html\n",
        "\n",
        "for the purpose of learning pytorch and torchaudio\n",
        "'''\n",
        "\n",
        "#%%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device= torch.device(\"cuda\" if torch.cuda.is_available() else \n",
        "                     \"cpu\")\n",
        "print(device)\n",
        "#%%\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "\n",
        "data_path= \"_dataset_\"\n",
        "# check the data availability\n",
        "# check if the data is already downloaded\n",
        "if not os.path.isdir(data_path):\n",
        "    print(\"Downloading data...\")\n",
        "    os.makedirs(data_path)\n",
        "    train_set= SPEECHCOMMANDS(data_path, download= True)\n",
        "    test_set= SPEECHCOMMANDS(data_path, download= True)\n",
        "    print(\"Downloading complete!\")\n",
        "else:\n",
        "    print(\"Data already downloaded!\")\n",
        "\n",
        "\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(data_path, download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "\n",
        "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
        "train_set= SubsetSC(\"training\")\n",
        "test_set=  SubsetSC(\"testing\")\n",
        "waveform, sample_rate, label, speaker_id, utterance_number= train_set[0]\n",
        "\n",
        "# show the size of training and testing split\n",
        "print(\"Training set size:\", len(train_set))\n",
        "print(\"Test set size:\", len(test_set))\n",
        "\n",
        "# show infomation about the data\n",
        "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
        "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "\n",
        "plt.plot(waveform.t().numpy())\n",
        "plt.show()\n",
        "\n",
        "print(\"Label: {}\".format(label))\n",
        "print(\"Speaker ID: {}\".format(speaker_id))\n",
        "print(\"Utterance number: {}\".format(utterance_number))\n",
        "\n",
        "#%% put info of the randomly chosed 10 data into a dataframe\n",
        "\n",
        "import pandas as pd\n",
        "waveform_pre10= []\n",
        "sample_rate_pre10= []\n",
        "label_pre10= []\n",
        "speaker_id_pre10= []\n",
        "utterance_number_pre10= []\n",
        "\n",
        "for j in range(10):\n",
        "    \n",
        "    # randomly choose one data from the training set\n",
        "    i= torch.randint(len(train_set), size= (1,)).item()\n",
        "\n",
        "    waveform_pre10.append(train_set[i][0].shape)\n",
        "    sample_rate_pre10.append(train_set[i][1])\n",
        "    label_pre10.append(train_set[i][2])\n",
        "    speaker_id_pre10.append(train_set[i][3])\n",
        "    utterance_number_pre10.append(train_set[i][4])\n",
        "\n",
        "df= pd.DataFrame({\"waveform.shape\": waveform_pre10,\n",
        "                    \"sample_rate\": sample_rate_pre10,\n",
        "                    \"label\": label_pre10,\n",
        "                    \"speaker_id\": speaker_id_pre10,\n",
        "                    \"utterance_number\": utterance_number_pre10})\n",
        "\n",
        "print(df)\n",
        "#%%\n",
        "# find all labels\n",
        "# it takes a while to run this cell, so we save the result to disk \n",
        "# or put it here as a comment\n",
        "\n",
        "# labels = sorted(list(set(datapoint[2] for datapoint in train_set)))\n",
        "# save the list of labels to disk for prediction later in the notebook\n",
        "# with open(\"labels.txt\", \"w\") as fp:\n",
        "#    fp.write(\"\\n\".join(labels))\n",
        "\n",
        "labels= [\n",
        " 'backward', 'bed',     'bird',     'cat',      'dog',\n",
        " 'down',    'eight',    'five',     'follow',   'forward',\n",
        " 'four',    'go',       'happy',    'house',    'learn',\n",
        " 'left',    'marvin',   'nine',     'no',       'off',\n",
        " 'on',      'one',      'right',    'seven',    'sheila',\n",
        " 'six',     'stop',     'three',    'tree',     'two',\n",
        " 'up',      'visual',   'wow',      'yes',      'zero'\n",
        "]\n",
        "\n",
        "#%%\n",
        "# downsampling the data to 8000Hz\n",
        "# why do we need to do this?\n",
        "# because the original sample rate is 16000Hz, which is too high for our model\n",
        "# we can downsample the data to 8000Hz to make the model run faster\n",
        "# and we can still hear the sound clearly\n",
        "\n",
        "waveform, sample_rate, label, speaker_id, utterance_number= train_set[100]\n",
        "\n",
        "new_sample_rate= 8_000\n",
        "transform= torchaudio.transforms.Resample(\n",
        "    orig_freq= sample_rate, \n",
        "    new_freq=  new_sample_rate)\n",
        "\n",
        "transformed_waveform= transform(waveform)\n",
        "\n",
        "ipd.Audio(waveform.numpy(),    rate=sample_rate)\n",
        "ipd.Audio(transformed_waveform.numpy(), rate=new_sample_rate)\n",
        "# %% encode/decode a label\n",
        "\n",
        "def label_to_index(word):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(word))\n",
        "\n",
        "def index_to_label(index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]\n",
        "\n",
        "lbl=  \"yes\"\n",
        "idx=  label_to_index(lbl)\n",
        "lbl2= index_to_label(idx)\n",
        "\n",
        "print(lbl, \"-->\", idx, \"-->\", lbl2)\n",
        "# %%\n",
        "\n",
        "# collate function\n",
        "# we need to collate the data to make sure all the data have the same size\n",
        "# we will pad the data with zeros to make them have the same size\n",
        "\n",
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch= [item.t() for item in batch]\n",
        "    batch= torch.nn.utils.rnn.pad_sequence(\n",
        "        batch, \n",
        "        batch_first=True, \n",
        "        padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    # A data tuple has the form:\n",
        "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for waveform, _, label, *_ in batch:\n",
        "        tensors += [waveform]\n",
        "        targets += [label_to_index(label)]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets\n",
        "\n",
        "batch_size = 1000 #1024 #256\n",
        "\n",
        "if device == \"cuda\":\n",
        "    num_workers = 1\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False\n",
        "\n",
        "#%%\n",
        "train_loader= torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "\n",
        "test_loader= torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory\n",
        ")\n",
        "\n",
        "# %%\n",
        "# we will put the data into the model in batches\n",
        "# each batch contains 1000 data\n",
        "# we will simulate the process here \n",
        "# (without training the model, \n",
        "# just to see how the data be pulled out from the dataloader)\n",
        "# we will print out the shape of the data in each batch\n",
        "# and the first 5 and last 5 labels in each batch\n",
        "\n",
        "for n, data in enumerate(train_loader):\n",
        "    w,l= data\n",
        "    print(n, w.shape, l[:5], l[-5:])\n",
        "\n",
        "for n, data in enumerate(test_loader):\n",
        "    w,l= data\n",
        "    print(n, w.shape, l[:5], l[-5:])\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "# %%\n",
        "\n",
        "## Define the Network\n",
        "\n",
        "'''\n",
        "For this tutorial we will use a convolutional neural network to process the raw audio data. \n",
        "\n",
        "Usually more advanced transforms are applied to the audio data, \n",
        "however CNNs can be used to accurately process the raw data.\n",
        "\n",
        "The specific architecture is modeled after the M5 network architecture\n",
        "described in [this paper](https://arxiv.org/pdf/1610.00087.pdf)_. \n",
        "An important aspect of models processing raw audio data is the receptive\n",
        "field of their first layer’s filters. \n",
        "\n",
        "Our model’s first filter is length 80 so when processing audio sampled at 8kHz \n",
        "the receptive field is around 10ms (and at 4kHz, around 20 ms). \n",
        "\n",
        "This size is similar to speech processing applications \n",
        "that often use receptive fields ranging from 20ms to 40ms.\n",
        "'''\n",
        "#%%\n",
        "class M5(nn.Module):\n",
        "    def __init__(self, \n",
        "                 n_input=   1, # number of input channel \n",
        "                 n_output= 35, # number of output classes\n",
        "                 kernel_size= 80, # length of the convolutional kernel\n",
        "                 stride=      16, # stride of the first convolutional layer\n",
        "                 n_channel=   32  # number of channels of the first convolutional layer\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1= nn.Conv1d(\n",
        "            n_input, \n",
        "            n_channel, \n",
        "            kernel_size= kernel_size, \n",
        "            stride=      stride)\n",
        "        \n",
        "        self.bn1=   nn.BatchNorm1d(n_channel)\n",
        "        self.pool1= nn.MaxPool1d(4)\n",
        "\n",
        "        self.conv2= nn.Conv1d(\n",
        "            n_channel, \n",
        "            n_channel, \n",
        "            kernel_size=3)\n",
        "        \n",
        "        self.bn2=   nn.BatchNorm1d(n_channel)\n",
        "        self.pool2= nn.MaxPool1d(4)\n",
        "        \n",
        "        self.conv3= nn.Conv1d(\n",
        "            n_channel, \n",
        "            2 * n_channel, \n",
        "            kernel_size=3)\n",
        "        \n",
        "        self.bn3=   nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3= nn.MaxPool1d(4)\n",
        "        \n",
        "        self.conv4= nn.Conv1d(\n",
        "            2 * n_channel, \n",
        "            2 * n_channel, \n",
        "            kernel_size=3)\n",
        "        \n",
        "        self.bn4=   nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4= nn.MaxPool1d(4)\n",
        "        \n",
        "        self.fc1=   nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x= self.conv1(x)\n",
        "        x= F.relu(self.bn1(x))\n",
        "        x= self.pool1(x)\n",
        "\n",
        "        x= self.conv2(x)\n",
        "        x= F.relu(self.bn2(x))\n",
        "        x= self.pool2(x)\n",
        "        \n",
        "        x= self.conv3(x)\n",
        "        x= F.relu(self.bn3(x))\n",
        "        x= self.pool3(x)\n",
        "        \n",
        "        x= self.conv4(x)\n",
        "        x= F.relu(self.bn4(x))\n",
        "        x= self.pool4(x)\n",
        "        \n",
        "        x= F.avg_pool1d(x, x.shape[-1])\n",
        "        x= x.permute(0, 2, 1)\n",
        "        x= self.fc1(x)\n",
        "        \n",
        "        y= F.log_softmax(x, dim=2)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "model= M5(n_input= transformed_waveform.shape[0], \n",
        "          n_output=len(labels))\n",
        "\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() \n",
        "               for p in model.parameters() \n",
        "               if p.requires_grad)\n",
        "\n",
        "n= count_parameters(model)\n",
        "print(\"Number of parameters: %s\" % n)\n",
        "# %%\n",
        "paras= model.parameters()\n",
        "\n",
        "# %%\n",
        "'''\n",
        "We will use the same optimization technique used in the paper, \n",
        "an Adam optimizer with weight decay set to 0.0001. \n",
        "\n",
        "At first, we will train with a learning rate of 0.01, \n",
        "but we will use a ``scheduler`` to decrease it to 0.001 \n",
        "during training after 20 epochs.\n",
        "'''\n",
        "\n",
        "optimizer= optim.Adam(\n",
        "    model.parameters(), \n",
        "    lr=0.01, \n",
        "    weight_decay=0.0001)\n",
        "\n",
        "scheduler= optim.lr_scheduler.StepLR(\n",
        "    optimizer, \n",
        "    step_size=20, \n",
        "    gamma= 0.1)  \n",
        "\n",
        "# reduce the learning after 20 epochs by a factor of 10\n",
        "\n",
        "\n",
        "# %%\n",
        "## Training and Testing the Network\n",
        "'''\n",
        "Now let’s define a training function that will feed our training data\n",
        "into the model and perform the backward pass and optimization steps. For\n",
        "training, the loss we will use is the negative log-likelihood. The\n",
        "network will then be tested after each epoch to see how the accuracy\n",
        "varies during the training.\n",
        "'''\n",
        "\n",
        "def train(model, epoch, log_interval):\n",
        "    model.train()\n",
        "    \n",
        "    losses= []\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # apply transform and model on whole batch directly on device\n",
        "        data = transform(data)\n",
        "        output = model(data)\n",
        "\n",
        "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
        "        loss = F.nll_loss(output.squeeze(), target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print training stats\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"\"\"\n",
        "            Train Epoch: {epoch} \n",
        "            [{batch_idx * len(data)}/{len(train_loader.dataset)} \n",
        "            ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\n",
        "            \"\"\")\n",
        "\n",
        "        # update progress bar\n",
        "        pbar.update(pbar_update)\n",
        "        # record loss\n",
        "        losses.append(loss.item())\n",
        "    \n",
        "    return losses\n",
        "\n",
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "\n",
        "def test(model, epoch):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # apply transform and model on whole batch directly on device\n",
        "        data = transform(data)\n",
        "        output = model(data)\n",
        "\n",
        "        pred = get_likely_index(output)\n",
        "        correct += number_of_correct(pred, target)\n",
        "\n",
        "        # update progress bar\n",
        "        pbar.update(pbar_update)\n",
        "\n",
        "    acc= correct / len(test_loader.dataset)\n",
        "\n",
        "    print(f\"\"\"\n",
        "    Test Epoch: {epoch}\n",
        "    Accuracy: {acc} ({correct}/{len(test_loader.dataset)}) \n",
        "    \"\"\")\n",
        "\n",
        "    return acc\n",
        "\n",
        "# %%\n",
        "log_interval= 100\n",
        "n_epoch=       2\n",
        "\n",
        "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
        "\n",
        "losses= []\n",
        "\n",
        "# The transform needs to live on the same device as the model and the data.\n",
        "transform= transform.to(device)\n",
        "\n",
        "with tqdm(total=n_epoch) as pbar:\n",
        "    for epoch in range(1, n_epoch + 1):\n",
        "        loss1= train(model, epoch, log_interval)\n",
        "        losses += [loss1]\n",
        "        test( model, epoch)\n",
        "        scheduler.step()\n",
        "print(losses)\n",
        "\n",
        "# %%\n",
        "# save model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "#%%\n",
        "# load model\n",
        "m5= M5(\n",
        "    n_input=  transformed_waveform.shape[0], \n",
        "    n_output= len(labels))\n",
        "m5.load_state_dict(torch.load(\"model.pth\"))\n",
        "\n",
        "# put the model to the device\n",
        "m5= m5.to(device)\n",
        "\n",
        "# test the model\n",
        "test(m5, 1)\n",
        "\n",
        "\n",
        "# %%\n",
        "# train one more epoch and then test again\n",
        "\n",
        "train(m5, 1, log_interval)\n",
        "test( m5, 1)\n",
        "\n",
        "\n",
        "# %%\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trian 10 more epochs and then test again\n",
        "\n",
        "for i in range(10):\n",
        "  train(m5, i, log_interval)\n",
        "\n",
        "test( m5, 1)\n"
      ],
      "metadata": {
        "id": "nU1EofhjKhwp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}