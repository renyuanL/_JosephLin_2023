{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNsniN/r76sytvO3gK71/51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renyuanL/_JosephLin_2023/blob/main/ryMLP001_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VwCaH7shbHq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Created on 2023-03-29\n",
        "Author: Renyuan Lyu\n",
        "Description: A simple MLP with 2 hidden layers\n",
        "Task: xor problem\n",
        "Path: Medium-Python-Neural-Network-master\\ryMLP001.py\n",
        "'''\n",
        "#%%\n",
        "import numpy as np\n",
        "\n",
        "# activation function and its derivative\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1-np.tanh(x)**2\n",
        "\n",
        "# loss function and its derivative (prime)\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2))\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size\n",
        "\n",
        "# Base class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input=  None\n",
        "        self.output= None\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# inherit from base class Layer\n",
        "class FCLayer(Layer):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights= np.random.rand(\n",
        "                        input_size, output_size\n",
        "                        ) - 0.5\n",
        "        self.bias=    np.random.rand(\n",
        "                        1, output_size\n",
        "                        ) - 0.5\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input=  input_data # a row vector\n",
        "        self.output= self.input @ self.weights + self.bias\n",
        "        return self.output\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_error, \n",
        "                             learning_rate):\n",
        "        \n",
        "        input_error=   output_error @ self.weights.T # dE/dX = dE/dY @ dY/dX\n",
        "        weights_error= self.input.T @ output_error   # dE/dW = dE/dY @ dY/dW\n",
        "\n",
        "        # dBias= output_error\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= weights_error * learning_rate\n",
        "        self.bias    -= output_error  * learning_rate\n",
        "\n",
        "        return input_error\n",
        "\n",
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation=       activation\n",
        "        self.activation_prime= activation_prime\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input=  input_data\n",
        "        self.output= self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_error, \n",
        "                             learning_rate):\n",
        "        input_error= output_error * self.activation_prime(self.input) \n",
        "        # element-wise multiplication \n",
        "        \n",
        "        return input_error\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, layers= []):\n",
        "        self.layers= layers\n",
        "        self.loss=   None\n",
        "        self.loss_prime= None\n",
        "\n",
        "        self.fig= plt.figure()\n",
        "        self.ax=  self.fig.add_subplot(1,1,1, title= 'network weights')\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layers):\n",
        "        self.layers += layers\n",
        "        return self\n",
        "    \n",
        "    def __add__(self, layers):\n",
        "        return self.add(layers)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.layers)\n",
        "    \n",
        "    def showLayerParams(self):\n",
        "        k= 0\n",
        "        for (l, layer) in enumerate(self.layers):\n",
        "            if isinstance(layer, FCLayer):\n",
        "                print(f'{k= }, {l= }\\n{layer.weights= },\\n{layer.bias= }\\n')\n",
        "            \n",
        "                # plot layer weights in line plot\n",
        "                '''\n",
        "                self.ax=  self.fig.add_subplot(1,2,k%(1*2)+1, \n",
        "                                               title= f'Layer {l= }')\n",
        "                self.ax.plot(layer.weights)\n",
        "                '''\n",
        "                k += 1\n",
        "                #plt.show()\n",
        "\n",
        "\n",
        "    # set loss to use\n",
        "    def setLoss(self, loss, loss_prime):\n",
        "        self.loss= loss\n",
        "        self.loss_prime= loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "        samples= len(input_data)\n",
        "        results= []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            \n",
        "            # forward propagation\n",
        "            output= input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output= layer.forward_propagation(\n",
        "                    output\n",
        "                    )\n",
        "\n",
        "            results += [output]\n",
        "\n",
        "        return results\n",
        "\n",
        "    # train the network\n",
        "    def _shuffle(self, x_train, y_train):\n",
        "        # sample dimension first\n",
        "        samples= len(x_train)\n",
        "        indices= np.arange(samples)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        x_train= x_train[indices]\n",
        "        y_train= y_train[indices]\n",
        "\n",
        "        return x_train, y_train\n",
        "    \n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            \n",
        "            x_train, y_train= self._shuffle(x_train, y_train)\n",
        "\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output= x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output= layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                error= self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error= layer.backward_propagation(\n",
        "                        error, learning_rate\n",
        "                        )\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "\n",
        "            print(f'{i= }, {err= }')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# decision boundary plot\n",
        "fig= plt.figure()\n",
        "#ax=  fig.add_subplot(111, projection=\"3d\")\n",
        "\n",
        "subplots=0\n",
        "\n",
        "def decision_boundary_plot(net, show= True):\n",
        "    #global fig, ax\n",
        "    #ax.clear()\n",
        "    global subplots\n",
        "    \n",
        "    \n",
        "    points= []\n",
        "    for x in np.linspace(-1, 1, 20):\n",
        "        for y in np.linspace(-1, 1, 20):\n",
        "            z= net.predict([[x, y]])\n",
        "            points += [(x, y, z[0][0,0])]\n",
        "\n",
        "    points= np.array(points)\n",
        "\n",
        "    #fig= plt.figure()\n",
        "    \n",
        "\n",
        "    ax= fig.add_subplot(\n",
        "        3, 4, \n",
        "        subplots%(3*4)+1, \n",
        "        projection= \"3d\",\n",
        "        title= f'epoch {subplots}'\n",
        "        )\n",
        "    subplots += 1\n",
        "\n",
        "    ax.scatter(\n",
        "        points[:, 0], \n",
        "        points[:, 1], \n",
        "        points[:, 2], \n",
        "        c= points[:, 2], \n",
        "        cmap= \"rainbow\")\n",
        "    \n",
        "    ax.view_init(azim= -100, elev= 20)\n",
        "\n",
        "    if show==True:\n",
        "        plt.show()\n",
        "    \n",
        "\n",
        "def XY_plot(X, Y, show= True):\n",
        "    '''\n",
        "    plot X,Y similar to that in decision_boundary_plot\n",
        "    '''\n",
        "\n",
        "    points= []\n",
        "    for x,y in zip(X,Y):\n",
        "        for i in range(len(x)):\n",
        "            points += [(x[i][0], x[i][1], y[i][0])]\n",
        "\n",
        "    points= np.array(points)\n",
        "\n",
        "    fig= plt.figure()\n",
        "\n",
        "    ax= fig.add_subplot(\n",
        "        1, 1, 1, \n",
        "        projection= \"3d\",\n",
        "        title= 'X,Y plot')\n",
        "    \n",
        "    ax.scatter(\n",
        "        points[:, 0], \n",
        "        points[:, 1], \n",
        "        points[:, 2], \n",
        "        c= points[:, 2], \n",
        "        cmap= \"rainbow\")\n",
        "    \n",
        "    ax.view_init(azim= -100, elev= +20)\n",
        "\n",
        "    if show==True:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    # training data\n",
        "    X= [[[-1, -1]], \n",
        "        [[-1, +1]], \n",
        "        [[+1, -1]], \n",
        "        [[+1, +1]]]\n",
        "    \n",
        "    Y= [[[+1]], \n",
        "        [[-1]], \n",
        "        [[-1]], \n",
        "        [[+1]]]\n",
        "    \n",
        "    X+=[[[-.1, -.1]], \n",
        "        [[-.1, +.1]], \n",
        "        [[+.1, -.1]], \n",
        "        [[+.1, +.1]]]\n",
        "    \n",
        "    Y+=[[[+1]], \n",
        "        [[-1]], \n",
        "        [[-1]], \n",
        "        [[+1]]]\n",
        "    \n",
        "\n",
        "    def generate_data(nsamples= 1_000, seed= 0):\n",
        "        \n",
        "        np.random.seed(seed)\n",
        "\n",
        "        X= []\n",
        "        Y= []\n",
        "        for i in range(nsamples):\n",
        "            x= np.random.rand(2) * 2 - 1\n",
        "            y= np.sign(x[0] * x[1])\n",
        "            y= np.array([y])\n",
        "            X += [[x]]\n",
        "            Y += [[y]]\n",
        "        \n",
        "        return X, Y\n",
        "    \n",
        "    X,Y= generate_data()\n",
        "\n",
        "    \n",
        "\n",
        "    X= np.array(X)\n",
        "    Y= np.array(Y)\n",
        "\n",
        "    XY_plot(X, Y, show= False)\n",
        "\n",
        "    # mlp network\n",
        "\n",
        "    mlp= Network([\n",
        "        FCLayer(2,5),  ActivationLayer(tanh,tanh_prime),\n",
        "        FCLayer(5,1),  ActivationLayer(tanh,tanh_prime)\n",
        "    ])\n",
        "    \n",
        "    mlp.showLayerParams()\n",
        "    decision_boundary_plot(mlp, show= False)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "    # train (fit)\n",
        "    mlp.setLoss(mse, mse_prime)\n",
        "    for i in range(10):\n",
        "        mlp.fit(\n",
        "            X, Y, \n",
        "            epochs= 20, \n",
        "            learning_rate= 0.02)\n",
        "\n",
        "        decision_boundary_plot(mlp, show= False)\n",
        "        #mlp.showLayerParams()\n",
        "    \n",
        "    mlp.showLayerParams()\n",
        "    decision_boundary_plot(mlp, show= True)\n",
        "    \n",
        "\n",
        "#%%\n",
        "\n",
        "#from   mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S0CJd8dH0GJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_EvMkirh0Hhp"
      }
    }
  ]
}