{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnSs97UMp9xf8O+uMCFapE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renyuanL/_JosephLin_2023/blob/main/_ry_01_recog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"/content/drive/MyDrive/_JosephLin2023/model.pt\"\n",
        "# \"/content/drive/MyDrive/_JosephLin2023/ryTest.wav\"\n",
        "\n",
        "#%%\n",
        "# get data from google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "#import sys\n",
        "#sys.path.append('/content/drive/MyDrive/_JosephLin2023/')\n",
        "\n",
        "# get data from link to google drive\n",
        "!gdown --id 1-3JF7rhFBpfajaIM-_NjKgg8WXHJ_fP9\n",
        "!gdown --id 1-3_AWSuw9m195PKgixouOR_2LDr_bAEE\n",
        "\n",
        "#Downloading...\n",
        "#From: https://drive.google.com/uc?id=1-3JF7rhFBpfajaIM-_NjKgg8WXHJ_fP9\n",
        "#To: /content/model.pt\n",
        "#100% 2.34M/2.34M [00:00<00:00, 149MB/s]\n",
        "\n",
        "#Downloading...\n",
        "#From: https://drive.google.com/uc?id=1-3_AWSuw9m195PKgixouOR_2LDr_bAEE\n",
        "#To: /content/ryTest.wav\n",
        "#100% 639k/639k [00:00<00:00, 142MB/s]\n"
      ],
      "metadata": {
        "id": "E0ZqNmSuWwQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgsoAn_paJFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMqoVutOOiZW"
      },
      "outputs": [],
      "source": [
        "\n",
        "#%%\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import sys\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "labels= [\n",
        " 'backward', 'bed',     'bird',     'cat',      'dog',\n",
        " 'down',    'eight',    'five',     'follow',   'forward',\n",
        " 'four',    'go',       'happy',    'house',    'learn',\n",
        " 'left',    'marvin',   'nine',     'no',       'off',\n",
        " 'on',      'one',      'right',    'seven',    'sheila',\n",
        " 'six',     'stop',     'three',    'tree',     'two',\n",
        " 'up',      'visual',   'wow',      'yes',      'zero'\n",
        "]\n",
        "\n",
        "# %%\n",
        "\n",
        "\n",
        "# %%\n",
        "def label_to_index(word):\n",
        "    # Return the position of the word in labels\n",
        "    return torch.tensor(labels.index(word))\n",
        "\n",
        "\n",
        "def index_to_label(index):\n",
        "    # Return the word corresponding to the index in labels\n",
        "    # This is the inverse of label_to_index\n",
        "    return labels[index]\n",
        "\n",
        "\n",
        "word_start = \"yes\"\n",
        "index = label_to_index(word_start)\n",
        "word_recovered = index_to_label(index)\n",
        "\n",
        "print(word_start, \"-->\", index, \"-->\", word_recovered)\n",
        "\n",
        "\n",
        "# %%\n",
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    # A data tuple has the form:\n",
        "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for waveform, _, label, *_ in batch:\n",
        "        tensors += [waveform]\n",
        "        targets += [label_to_index(label)]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "#%%\n",
        "\n",
        "class ryM(nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_chs=   1,  #  1 channel, mono waveform\n",
        "                 out_cls= 35,  # 35 words as output classes\n",
        "                 sample_rate=  16_000 # sample rate of the audio file\n",
        "                 ):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        new_sample_rate= sample_rate//2 #8_000\n",
        "\n",
        "        self.transform= torchaudio.transforms.Resample(\n",
        "            orig_freq= sample_rate, \n",
        "            new_freq=  new_sample_rate)\n",
        "\n",
        "        self.act=  nn.ReLU()\n",
        "        self.flat= nn.Flatten()\n",
        "        self.out=  nn.LogSoftmax(dim=-1)\n",
        "        #self.out=  nn.Softmax(dim=-1)\n",
        "\n",
        "        k1= int(.02* new_sample_rate) # 160 # 20ms\n",
        "        s1= int(.01* new_sample_rate) #  80 # 10ms\n",
        "        ch1= 64 # 64 channels in 1st convolution layer\n",
        "\n",
        "        k2= 4 # kernel size in the other conv layer\n",
        "        s2= 2 # stride in the other conv layer\n",
        "\n",
        "        self.conv1= nn.Conv1d(in_chs, ch1,   kernel_size= k1, stride= s1) \n",
        "        self.bn1=   nn.BatchNorm1d(ch1)\n",
        "\n",
        "        self.conv2= nn.Conv1d(ch1,  ch1 *2,  kernel_size= k2, stride= s2)\n",
        "        self.bn2=   nn.BatchNorm1d(ch1 *2)\n",
        "\n",
        "        self.conv3= nn.Conv1d(ch1 *2, ch1 *4, kernel_size= k2, stride= s2)\n",
        "        self.bn3=   nn.BatchNorm1d(ch1 *4)\n",
        "\n",
        "        self.conv4= nn.Conv1d(ch1 *4, ch1 *4, kernel_size= k2, stride= s2)\n",
        "        self.bn4=   nn.BatchNorm1d(ch1 *4)\n",
        "\n",
        "        self.conv5= nn.Conv1d(ch1 *4, ch1 *2, kernel_size= k2, stride= s2)\n",
        "        self.bn5=   nn.BatchNorm1d(ch1 *2)\n",
        "        \n",
        "        self.fc1= nn.Linear(ch1 *2, ch1)\n",
        "        self.fc2= nn.Linear(ch1,    out_cls)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x= self.transform(x) # (1,16000) -> (1,8000) # downsample by factor of 2\n",
        "\n",
        "        #  CNNs\n",
        "        x= self.conv1(x) # (1, 8000) -> (64, 99)\n",
        "        x= self.bn1(x)   \n",
        "        x= self.act(x)   \n",
        "        \n",
        "        x= self.conv2(x) # (64, 99) -> (128, 48)\n",
        "        x= self.bn2(x)   \n",
        "        x= self.act(x)   \n",
        "        \n",
        "        x= self.conv3(x) # (128, 48) -> (256, 23)\n",
        "        x= self.bn3(x)   \n",
        "        x= self.act(x)   \n",
        "       \n",
        "        x= self.conv4(x) # (256, 23) -> (256, 10)\n",
        "        x= self.bn4(x)   \n",
        "        x= self.act(x)\n",
        "\n",
        "        x= self.conv5(x) # (256, 10) -> (128, 4)\n",
        "        x= self.bn5(x)   \n",
        "        x= self.act(x)   \n",
        "        \n",
        "        # global average pooling\n",
        "        x= F.avg_pool1d(x, x.shape[-1])  # -> (128, 1)\n",
        "        x= self.flat(x) # -> (128)\n",
        "\n",
        "        # MLPs\n",
        "        x= self.fc1(x)  # -> (64)\n",
        "        x= self.act(x)  # -> (64)\n",
        "\n",
        "        x= self.fc2(x)  # -> (35)\n",
        "        y= self.out(x)  # -> (35)\n",
        "\n",
        "        return y\n",
        "\n",
        "#model= ryM(in_chs= 1, out_cls=35)\n",
        "\n",
        "# ryM, Test@epoch= 13, acc=【0.8706】, [9581/11005]\n",
        "\n",
        "# Train@epoch= 15, Loss: 0.205410\n",
        "# ryM,  Test@epoch= 15, acc= 0.8642, [9510/11005]\n",
        "\n",
        "# ryM,  Test@epoch= 15, acc= 0.8531, [9388/11005]\n",
        "# M6,   Test Epoch: 24  Accuracy: 9362/11005 (85%)\n",
        "# M5_1, Test Epoch: 21  Accuracy: 8905/11005 (81%)\n",
        "# %%\n",
        "\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "\n",
        "# initialize the model\n",
        "mdl= ryM(in_chs= 1, out_cls=35)\n",
        "mdl.to(device)\n",
        "\n",
        "# load the weights\n",
        "\n",
        "mdl.load_state_dict(\n",
        "    torch.load('model.pt', map_location=torch.device('cpu')\n",
        "    ))\n",
        "\n",
        "# only in inference mode\n",
        "mdl.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# %%\n",
        "# get the test data set to test the model on\n",
        "\n",
        "# get the test data set\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "\n",
        "data_path= \"./\"\n",
        "# check if the dircetory exists, if not, make it\n",
        "if not os.path.isdir(data_path):\n",
        "    os.mkdir(data_path)\n",
        "\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(data_path, download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as fileobj:\n",
        "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
        "test_set= SubsetSC(\"testing\")\n",
        "waveform, sample_rate, label, speaker_id, utterance_number = test_set[0]\n",
        "len(test_set) # 11_005\n",
        "\n",
        "# %%\n",
        "# put the test data into a data loader\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    # A data tuple has the form:\n",
        "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for waveform, _, label, *_ in batch:\n",
        "        tensors += [waveform]\n",
        "        targets += [label_to_index(label)]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets = torch.stack(targets)\n",
        "\n",
        "    return tensors, targets\n",
        "\n",
        "\n",
        "batch_size= 2048\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory= False #True # CUDA only, much faster\n",
        ")\n",
        "\n",
        "len(test_loader)\n",
        "\n",
        "# using the test data loader, test the model\n",
        "\n",
        "def number_of_correct(pred, target):\n",
        "    # count number of correct predictions\n",
        "    return pred.squeeze().eq(target).sum().item()\n",
        "\n",
        "def get_likely_index(tensor):\n",
        "    # find most likely label index for each element in the batch\n",
        "    return tensor.argmax(dim=-1)\n",
        "\n",
        "def test(model, test_loader, epoch=0):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "\n",
        "        data=   data.to(device)\n",
        "        target= target.to(device)\n",
        "\n",
        "        # apply transform and model on whole batch directly on device\n",
        "        # data = transform(data)\n",
        "        output= model(data)\n",
        "\n",
        "        pred = get_likely_index(output)\n",
        "        correct += number_of_correct(pred, target)\n",
        "\n",
        "        # update progress bar\n",
        "        #pbar.update(pbar_update)\n",
        "\n",
        "    acc= correct/len(test_loader.dataset)\n",
        "    print(f\"\\nTest@{epoch= }, acc=【{acc:.4f}】, [{correct}/{len(test_loader.dataset)}]\\n\")\n",
        "    \n",
        "    return acc\n",
        "\n",
        "#%%\n",
        "# check the speed of the model\n",
        "\n",
        "import time\n",
        "\n",
        "t0= time.time()\n",
        "acc= test(mdl, test_loader)\n",
        "t1= time.time()\n",
        "\n",
        "print(f\"{t1-t0= :.4f} seconds\")\n",
        "print(f'{acc= :.4f}')\n",
        "\n",
        "#%%\n",
        "\n",
        "x, _, y, *_= test_loader.dataset[1001]\n",
        "x= x.squeeze()\n",
        "x,y\n",
        "\n",
        "# %%\n",
        "# get the waveforms from the currrent dircetory\n",
        "# and test the model on them\n",
        "\n",
        "# get the waveform from ryTest.wav\n",
        "# and test the model on it\n",
        "# ryTest.wav is a recording of the words of several words\n",
        "# the words are: \"\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", ...\n",
        "\n",
        "import torchaudio\n",
        "\n",
        "# load the waveform\n",
        "\n",
        "# get the directory of the current file\n",
        "dir= '.' #os.path.dirname(os.path.abspath(__file__))\n",
        "\n",
        "# get the path to the file\n",
        "fn= \"ryTest.wav\"\n",
        "path= os.path.join(dir, fn)\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "#%%\n",
        "# %%\n",
        "# get the waveforms from the currrent dircetory\n",
        "# and test the model on them\n",
        "\n",
        "# get the waveform from ryTest.wav\n",
        "# and test the model on it\n",
        "# ryTest.wav is a recording of the words of several words\n",
        "# the words are: \n",
        "# \"zero\", \"one\", \"two\", \"three\", \"four\", \n",
        "# \"five\", \"six\", \"seven\", \"eight\", \"nine\", \n",
        "# \"forward\", \"backward\", \n",
        "# \"up\", \"down\", \"left\", \"right\",\n",
        "\n",
        "\n",
        "\n",
        "# load the waveform\n",
        "waveform, sample_rate= torchaudio.load(path)\n",
        "\n",
        "# plot the waveform\n",
        "plt.figure()\n",
        "plt.plot(waveform.t().numpy())\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# segment the waveform into words\n",
        "# segment the waveform into 1 second segments\n",
        "\n",
        "# get the number of samples in 1 second\n",
        "n_samples= sample_rate\n",
        "\n",
        "# get the number of samples in the waveform\n",
        "n_samples_waveform= waveform.shape[1]\n",
        "\n",
        "# get the number of segments\n",
        "n_segments= n_samples_waveform//n_samples\n",
        "\n",
        "# get the segments\n",
        "segments= torch.split(waveform, n_samples, dim=1)\n",
        "\n",
        "# plot the segments\n",
        "plt.figure()\n",
        "for i in range(n_segments):\n",
        "    plt.subplot(n_segments, 1, i+1)\n",
        "    plt.plot(segments[i].t().numpy())\n",
        "plt.show()\n",
        "\n",
        "# %%\n",
        "# use the model to predict the words in the segments\n",
        "\n",
        "# %%\n",
        "# stack the segments into a batch of tensors\n",
        "# discard the last segment if it is not of size n_samples\n",
        "# or pad it with zeros if it is smaller than n_samples\n",
        "\n",
        "# get the number of samples in the last segment\n",
        "n_samples_last_segment= segments[-1].shape[1]\n",
        "\n",
        "# check if the last segment is of size n_samples\n",
        "if n_samples_last_segment != n_samples:\n",
        "    # pad the last segment with zeros\n",
        "    seg= torch.cat((\n",
        "        segments[-1], torch.zeros(\n",
        "            (1, n_samples-n_samples_last_segment))), \n",
        "        dim=1 \n",
        "        )\n",
        "#%%\n",
        "# stack the segments into a batch of tensors\n",
        "sss= torch.stack([*segments[:-1], seg], dim=0)\n",
        "\n",
        "# %%\n",
        "# use the model to predict the words in the segs\n",
        "\n",
        "sss_cuda= sss.to(device)\n",
        "\n",
        "# get the prediction\n",
        "pred= mdl(sss_cuda)\n",
        "# get the most likely index\n",
        "pred= get_likely_index(pred)\n",
        "# get the word\n",
        "result= [labels[p] for p in pred]\n",
        "\n",
        "print(f'{result= }')\n",
        "\n",
        "# plot the segments\n",
        "plt.figure()\n",
        "for i in range(n_segments):\n",
        "    plt.subplot(n_segments//2+1, 2, i+1)\n",
        "    plt.plot(sss[i].t().numpy())\n",
        "    plt.title(result[i])\n",
        "plt.show()\n",
        "# %%\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# %%\n"
      ],
      "metadata": {
        "id": "P21vYBiiSQdW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}