import numpy as np

# activation function and its derivative
def tanh(x):
    return np.tanh(x)

def tanh_prime(x):
    '''
    tanh'(x) = 1 - tanh(x)^2
    '''
    return 1-np.tanh(x)**2


# the following are automatically generated by co-pilot
def sigmoid(x):
    return 1/(1+np.exp(-x))

def sigmoid_prime(x):
    return sigmoid(x)*(1-sigmoid(x))

def relu(x):
    return np.maximum(0,x)

def relu_prime(x):
    return np.where(x>0, 1, 0)

def softmax(x):
    '''
    Compute softmax values for each sets of scores in x.
    It calculates the exponential of each element in the input array x, 
    subtracts the maximum value in x for **numerical stability**, 
    and then normalizes the result so that the sum of the resulting array is 1.
    '''
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum()

def softmax_prime(x):
    return softmax(x)*(1-softmax(x))

def linear(x):
    return x

def linear_prime(x):
    return 1

def leaky_relu(x):
    return np.where(x>0, x, 0.01*x)

def leaky_relu_prime(x):
    return np.where(x>0, 1, 0.01)

def elu(x):
    return np.where(x>0, x, 0.01*(np.exp(x)-1))

def elu_prime(x):
    return np.where(x>0, 1, elu(x)+1)

def selu(x):
    return np.where(x>0, 1.0507*x, 1.0507*1.67326*(np.exp(x)-1))

def selu_prime(x):
    return np.where(x>0, 1.0507, 1.0507*1.67326*elu(x))

def gelu(x):
    return 0.5*x*(1+np.tanh(np.sqrt(2/np.pi)*(x+0.044715*np.power(x,3))))

def gelu_prime(x):
    return 0.5*(1+np.tanh(np.sqrt(2/np.pi)*(x+0.044715*np.power(x,3))))+0.5*x*(1/np.cosh(np.sqrt(2/np.pi)*(x+0.044715*np.power(x,3))))*(np.sqrt(2/np.pi)*(1+0.134145*np.power(x,2)))


