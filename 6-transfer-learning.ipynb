{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained models and transfer learning\n",
        "\n",
        "Training CNNs can take a lot of time, and a lot of data is required for that task. However, much of the time is spent to learn the best low-level filters that a network is using to extract patterns from images. A natural question arises - can we use a neural network trained on one dataset and adapt it to classifying different images without full training process?\n",
        "\n",
        "This approach is called **transfer learning**, because we transfer some knowledge from one neural network model to another. In transfer learning, we typically start with a pre-trained model, which has been trained on some large image dataset, such as **ImageNet**. Those models can already do a good job extracting different features from generic images, and in many cases just building a classifier on top of those extracted features can yield a good result."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/pytorchcv.py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/requirements.txt"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchinfo import summary\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from pytorchcv import train, plot_results, display_dataset, train_long, check_image_dir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cats vs. Dogs Dataset\n",
        "\n",
        "In this unit, we will solve a real-life problem of classifying images of cats and dogs. For this reason, we will use [Kaggle Cats vs. Dogs Dataset](https://www.kaggle.com/c/dogs-vs-cats), which can also be downloaded [from Microsoft](https://www.microsoft.com/en-us/download/details.aspx?id=54765).\n",
        "\n",
        "Let's download this dataset and extract it into `data` directory (this process may take some time!):"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('data/kagglecatsanddogs_5340.zip'):\n",
        "    !wget -P data -q https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "if not os.path.exists('data/PetImages'):\n",
        "    with zipfile.ZipFile('data/kagglecatsanddogs_5340.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('data')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, there are some corrupt image files in the dataset. We need to do quick cleaning to check for corrupted files. In order not to clobber this tutorial, we moved the code to verify dataset into a module, and we will just call it here. `check_image_dir` goes over the whole dataset image by image, tries to load the image and check if it can be loaded correctly. All corrupt images are deleted."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "check_image_dir('data/PetImages/Cat/*.jpg')\n",
        "check_image_dir('data/PetImages/Dog/*.jpg')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's load the images into PyTorch dataset, converting them to tensors and doing some normalization. We define image transformation pipeline by composing several primitive transformations using `Compose`:\n",
        "* `Resize` resizes our image to 256x256 dimensions\n",
        "* `CenterCrop` gets the central part of the image with size 224x224. Pre-trained VGG network has been trained on 224x224 images, thus we need to bring our dataset to this size.\n",
        "* `ToTensor` normalizes pixel intensities to be in 0..1 range, and convert images to PyTorch tensors\n",
        "* `std_normalize` transform is additional normalization step specific for VGG network. When training a VGG network, original images from ImageNet were transformed by subtracting dataset mean intensity by color and dividing by standard deviation (also by color). Thus, we need to apply the same transformation to our dataset, so that all images are processed correctly. \n",
        "\n",
        "There are a few reason why we resized images to size 256, and then cropped to 224 pixels:\n",
        "* We wanted to demonstrate more possible transformations.\n",
        "* Pets are usually somewhere in the central part of the image, so we can improve classification by focusing more on the central part\n",
        "* Since some of the images are not square, we end up having padded parts of the image that do not contain any useful picture data, and cropping the image a bit reduces the padded part."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "std_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                          std=[0.229, 0.224, 0.225])\n",
        "trans = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(), \n",
        "        std_normalize])\n",
        "dataset = torchvision.datasets.ImageFolder('data/PetImages',transform=trans)\n",
        "trainset, testset = torch.utils.data.random_split(dataset,[20000,len(dataset)-20000])\n",
        "\n",
        "display_dataset(dataset)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "## Pre-trained models\n",
        "\n",
        "There are many different pre-trained models available inside `torchvision` module, and even more models can be found on the Internet. Let's see how simplest VGG-16 model can be loaded and used.  First, we'll download the weights for the VGG-16 model that we stored in a local repository."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Download model weights in the sandbox environment\n",
        "!mkdir -p models\n",
        "!wget -P models https://github.com/MicrosoftDocs/pytorchfundamentals/raw/main/computer-vision-pytorch/vgg16-397923af.pth"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Next, we'll load the weights into the pre-trained VGG-16 model by using the `load_state_dict` method.  Then, use the `eval` method to set the model to inference mode."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'models/vgg16-397923af.pth'\n",
        "\n",
        "vgg = torchvision.models.vgg16()\n",
        "vgg.load_state_dict(torch.load(file_path))\n",
        "vgg.eval()\n",
        "\n",
        "sample_image = dataset[0][0].unsqueeze(0)\n",
        "res = vgg(sample_image)\n",
        "print(res[0].argmax())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result that we have received is a number of an `ImageNet` class, which can be looked up [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). We can use the following code to automatically load this class table and return the result:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json, requests\n",
        "class_map = json.loads(requests.get(\"https://raw.githubusercontent.com/MicrosoftDocs/pytorchfundamentals/main/computer-vision-pytorch/imagenet_class_index.json\").text)\n",
        "class_map = { int(k) : v for k,v in class_map.items() }\n",
        "\n",
        "class_map[res[0].argmax().item()]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also see the architecture of the VGG-16 network:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vgg,input_size=(1,3,224,224))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to the layer we already know, there is also another layer type called **Dropout**. These layers act as **regularization** technique. Regularization makes slight modifications to the learning algorithm so the model generalizes better. During training, dropout layers discard some proportion (around 30%) of the neurons in the previous layer, and training happens without them. This helps to get the optimization process out of local minima, and to distribute decisive power between different neural paths, which improves overall stability of the network.\n",
        "\n",
        "## GPU computations\n",
        "\n",
        "Deep neural networks, such as VGG-16 and other more modern architectures require quite a lot of computational power to run. It makes sense to use GPU acceleration, if it is available. In order to do so, we need to explicitly move all tensors involved in the computation to GPU.\n",
        "\n",
        "The way it is normally done is to check the availability of GPU in the code, and define `device` variable that points to the computational device - either GPU or CPU.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print('Doing computations on device = {}'.format(device))\n",
        "\n",
        "vgg.to(device)\n",
        "sample_image = sample_image.to(device)\n",
        "\n",
        "vgg(sample_image).argmax()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Extracting VGG features\n",
        "\n",
        "If we want to use VGG-16 to extract features from our images, we need the model without final classification layers. In fact, this \"feature extractor\" can be obtained using `vgg.features` method:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "res = vgg.features(sample_image).cpu()\n",
        "plt.figure(figsize=(15,3))\n",
        "plt.imshow(res.detach().view(-1,512))\n",
        "print(res.size())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dimension of feature tensor is 512x7x7, but in order to visualize it we had to reshape it to 2D form.\n",
        "\n",
        "Now let's try to see if those features can be used to classify images. Let's manually take some portion of images (800 in our case), and pre-compute their feature vectors. We will store the result in one big tensor called `feature_tensor`, and also labels into `label_tensor`:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 8\n",
        "dl = torch.utils.data.DataLoader(dataset,batch_size=bs,shuffle=True)\n",
        "num = bs*100\n",
        "feature_tensor = torch.zeros(num,512*7*7).to(device)\n",
        "label_tensor = torch.zeros(num).to(device)\n",
        "i = 0\n",
        "for x,l in dl:\n",
        "    with torch.no_grad():\n",
        "        f = vgg.features(x.to(device))\n",
        "        feature_tensor[i:i+bs] = f.view(bs,-1)\n",
        "        label_tensor[i:i+bs] = l\n",
        "        i+=bs\n",
        "        print('.',end='')\n",
        "        if i>=num:\n",
        "            break\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define `vgg_dataset` that takes data from this tensor, split it into training and test sets using `random_split` function, and train a small one-layer dense classifier network on top of extracted features:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_dataset = torch.utils.data.TensorDataset(feature_tensor,label_tensor.to(torch.long))\n",
        "train_ds, test_ds = torch.utils.data.random_split(vgg_dataset,[700,100])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_ds,batch_size=32)\n",
        "test_loader = torch.utils.data.DataLoader(test_ds,batch_size=32)\n",
        "\n",
        "net = torch.nn.Sequential(torch.nn.Linear(512*7*7,2),torch.nn.LogSoftmax()).to(device)\n",
        "\n",
        "history = train(net,train_loader,test_loader)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result is great, we can distinguish between a cat and a dog with almost 98% probability! However, we have only tested this approach on a small subset of all images, because manual feature extraction seems to take a lot of time.\n",
        "\n",
        "## Transfer learning using one VGG network\n",
        "\n",
        "We can also avoid manually pre-computing the features by using the original VGG-16 network as a whole during training. Let's look at the VGG-16 object structure:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(vgg)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that the network contains:\n",
        "* feature extractor (`features`), comprised of a number of convolutional and pooling layers\n",
        "* average pooling layer (`avgpool`)\n",
        "* final `classifier`, consisting of several dense layers, which turns 25088 input features into 1000 classes (which is the number of classes in ImageNet)\n",
        "\n",
        "To train the end-to-end model that will classify our dataset, we need to:\n",
        "* **replace the final classifier** with the one that will produce required number of classes. In our case, we can use one `Linear` layer with 25088 inputs and 2 output neurons.\n",
        "* **freeze weights of convolutional feature extractor**, so that they are not trained. It is recommended to initially do this freezing, because otherwise untrained classifier layer can destroy the original pre-trained weights of convolutional extractor. Freezing weights can be accomplished by setting `requires_grad` property of all parameters to `False`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vgg.classifier = torch.nn.Linear(25088,2).to(device)\n",
        "\n",
        "for x in vgg.features.parameters():\n",
        "    x.requires_grad = False\n",
        "\n",
        "summary(vgg,(1, 3,244,244))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from the summary, this model contain around 15 million total parameters, but only 50k of them are trainable - those are the weights of classification layer. That is good, because we are able to fine-tune smaller number of parameters with smaller number of examples.\n",
        "\n",
        "Now let's train the model using our original dataset. This process will take a long time, so we will use the `train_long` function that will print some intermediate results without waiting for the end of epoch. It is highly recommended to run this training on GPU-enabled compute!\n",
        "\n",
        "> **Note:** If you are interested in the implementation of the `train_long` function, refer to the `pytorchcv.py` file."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, testset = torch.utils.data.random_split(dataset,[20000,len(dataset)-20000])\n",
        "train_loader = torch.utils.data.DataLoader(trainset,batch_size=16)\n",
        "test_loader = torch.utils.data.DataLoader(testset,batch_size=16)\n",
        "\n",
        "train_long(vgg,train_loader,test_loader,loss_fn=torch.nn.CrossEntropyLoss(),epochs=1,print_freq=90)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like we have obtained reasonably accurate cats vs. dogs classifier! Let's save it for future use!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vgg,'data/cats_dogs.pth')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then load the model from file at any time. You may find it useful in case the next experiment destroys the model - you would not have to re-start from scratch."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vgg = torch.load('data/cats_dogs.pth')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning transfer learning\n",
        "\n",
        "In the previous section, we have trained the final classifier layer to classify images in our own dataset. However, we did not re-train the feature extractor, and our model relied on the features that the model has learned on ImageNet data. If your objects visually differ from ordinary ImageNet images, this combination of features might not work best. Thus it makes sense to start training convolutional layers as well.\n",
        "\n",
        "To do that, we can unfreeze the convolutional filter parameters that we have previously frozen. \n",
        "\n",
        "> **Note:** It is important that you freeze parameters first and perform several epochs of training in order to stabilize weights in the classification layer. If you immediately start training end-to-end network with unfrozen parameters, large errors are likely to destroy the pre-trained weights in the convolutional layers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for x in vgg.features.parameters():\n",
        "    x.requires_grad = True"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After unfreezing, we can do a few more epochs of training. You can also select lower learning rate, in order to minimize the impact on the pre-trained weights. However, even with low learning rate, you can expect the accuracy to drop in the beginning of the training, until finally reaching slightly higher level than in the case of fixed weights.\n",
        "\n",
        "> **Note:** This training happens much slower, because we need to propagate gradients back through many layers of the network! You may want to watch the first few minibatches to see the tendency, and then stop the computation."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "train_long(vgg,train_loader,test_loader,loss_fn=torch.nn.CrossEntropyLoss(),epochs=1,print_freq=90,lr=0.0001)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other computer vision models\n",
        "\n",
        "VGG-16 is one of the simplest computer vision architectures. `torchvision` package provides many more pre-trained networks. The most frequently used ones among those are **ResNet** architectures, developed by Microsoft, and **Inception** by Google. For example, let's explore the architecture of the simplest ResNet-18 model (ResNet is a family of models with different depth, you can try experimenting with ResNet-151 if you want to see what a really deep model looks like):"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "resnet = torchvision.models.resnet18()\n",
        "print(resnet)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "azureml_py38_pt_and_tf"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model contains the same building blocks: feature extractor and final classifier (`fc`). This allows us to use this model in exactly the same manner as we have been using VGG-16 for transfer learning. You can try experimenting with the code above, using different ResNet models as the base model, and see how accuracy changes.\n",
        "\n",
        "## Batch Normalization\n",
        "\n",
        "This network contains yet another type of layer: **Batch Normalization**. The idea of batch normalization is to bring values that flow through the neural network to right interval. Usually neural networks work best when all values are in the range of [-1,1] or [0,1], and that is the reason that we scale/normalize our input data accordingly. However, during training of a deep network, it can happen that values get significantly out of this range, which makes training problematic. Batch normalization layer computes average and standard deviation for all values of the current minibatch, and uses them to normalize the signal before passing it through a neural network layer. This significantly improves the stability of deep networks.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaway\n",
        "\n",
        "Using transfer learning, we were able to quickly put together a classifier for our custom object classification task, and achieve high accuracy. However, this example was not completely fair, because original VGG-16 network was pre-trained to recognize cats and dogs, and thus we were just reusing most of the patterns that were already present in the network. You can expect lower accuracy on more exotic domain-specific objects, such as details on production line in a plant, or different tree leaves.\n",
        "\n",
        "You can see that more complex tasks that we are solving now require higher computational power, and cannot be easily solved on the CPU. In the next unit, we will try to use more lightweight implementation to train the same model using lower compute resources, which results in just slightly lower accuracy. "
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "py38_default"
    },
    "kernelspec": {
      "display_name": "py38_default",
      "language": "python",
      "name": "conda-env-py38_default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "conda-env-py38_default-py",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "e06f4fac4d5086440e18d888631bc810f8cfacdcf722ebe17f49a2bb6dfba1af"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}