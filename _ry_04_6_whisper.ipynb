{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMw9AzdnRNfREsFQIl7DaOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renyuanL/_JosephLin_2023/blob/main/_ry_04_6_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper\n",
        "!wget https://github.com/renyuanL/_JosephLin_2023/raw/main/ry35words.wav"
      ],
      "metadata": {
        "id": "0uNVbQWXz6-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xgge9Kvzj3m"
      },
      "outputs": [],
      "source": [
        "\n",
        "#%%\n",
        "fn_wav= fn_wav_00= 'ry35words.wav'\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import whisper\n",
        "import whisper as wp\n",
        "\n",
        "device= ('cuda' if torch.cuda.is_available() else \n",
        "         'cpu')\n",
        "\n",
        "def get_md(model_size= 'tiny'):\n",
        "    md= whisper.load_model(model_size)\n",
        "    md= md.to(device)\n",
        "    return md\n",
        "\n",
        "def get_wav(fn_wav= 'ry35words.wav'):\n",
        "    wav, sr= torchaudio.load(fn_wav)\n",
        "    wav= wav.squeeze()\n",
        "    wav= wav.to(device)\n",
        "    return wav\n",
        "#%% the simplest way to run the model\n",
        "md= get_md()\n",
        "\n",
        "\n",
        "x=  get_wav()\n",
        "y=  md.transcribe(x)\n",
        "print(f'{y= }')\n",
        "\n",
        "#%% 研究一下 tokenizer ...\n",
        "\n",
        "# (50258, 50259, 50359)\n",
        "# y= '<|startoftranscript|><|en|><|transcribe|>'\n",
        "\n",
        "tok= torch.tensor(\n",
        "   [[50258, 50259, 50359]]\n",
        "   )\n",
        "\n",
        "def txt4tok(tok):\n",
        "  txt= wp.tokenizer.get_encoding(\n",
        "     'multilingual'\n",
        "     ).decode(tok[0].tolist())\n",
        "  return txt\n",
        "\n",
        "def tok4txt(txt):\n",
        "  tok= wp.tokenizer.get_encoding(\n",
        "     'multilingual'\n",
        "     ).encode(txt)\n",
        "  tok= torch.tensor(tok)[None, ...]\n",
        "  return tok\n",
        "\n",
        "txt2tok= tok4txt # txt->tok, tok<-txt\n",
        "tok2txt= txt4tok # tok->txt, txt<-tok\n",
        "\n",
        "txt= '0, 1, 2, 3, a, b, c, d, 一, 乙, 丁, 七.'\n",
        "tok=  tok4txt(txt)\n",
        "txt1= txt4tok(tok)\n",
        "\n",
        "\n",
        "txt, tok, txt1\n",
        "'''\n",
        "tensor([[   \n",
        "  15,     11,   \n",
        "  502,    11,   \n",
        "  568,    11,  \n",
        "  805,    11,   \n",
        "  \n",
        "  257,    11,\n",
        "  272,    11,   \n",
        "  269,    11,   \n",
        "  274,    11, \n",
        "  \n",
        "  26923,          11,   220,  \n",
        "  2930,   247,    11,   220,   \n",
        "  940,    223,    11,   220, \n",
        "  29920,          13\n",
        "]]),\n",
        "'''\n",
        "\n",
        "txt= 'a, b, c, d, ant, bug, cat, dog, 蟻, 蟲, 貓, 狗.'\n",
        "tok=  tok4txt(txt)\n",
        "tok\n",
        "\n",
        "'''\n",
        "tensor([[   \n",
        "  64,     11,   \n",
        "  272,    11,   \n",
        "  269,    11,   \n",
        "  274,    11,  \n",
        "  \n",
        "  2511,    11,\n",
        "  7426,    11,  \n",
        "  3857,    11,  \n",
        "  3000,    11,   220,   \n",
        "  \n",
        "  164,     253,   119,  11,  220,   \n",
        "  164,     253,   110,  11,  220, \n",
        "  11561,   241,         11,  220, \n",
        "  18637,   245,         13\n",
        "  ]])\n",
        "'''\n",
        "\n",
        "#%% dive into the model\n",
        "\n",
        "print(f'{md.dims= }, {md= }')\n",
        "\n",
        "theDims= '''\n",
        "ModelDimensions(\n",
        "  n_mels= 80, # 聲音的mel頻譜的維度， sampleRate= 16KHz \n",
        "  n_audio_ctx=1500, # 固定的音長，1500 = 30 sec \n",
        "  \n",
        "  n_audio_state=384, # 384 = 6*64, 6 heads, 1 head = 64 維\n",
        "  n_audio_head=6, \n",
        "  n_audio_layer=4, \n",
        "  \n",
        "  n_vocab= 51_865,  # 詞彙表的大小 # 是 相異 tok 的數量 \n",
        "  n_text_ctx=448,   # 30 sec 內，最大的 tok 長度，448 = 224*2, 224 似乎是「詞」數量的上限\n",
        "\n",
        "  n_text_state=384, \n",
        "  n_text_head=6, \n",
        "  n_text_layer=4\n",
        "'''\n",
        "\n",
        "# 發現 whisper 原始的 model 有一個 bug, 可能不影響辨認結果，或者影響很小，但是還是要修正一下。\n",
        "#\n",
        "# 它原來的 code 是這樣的：這應是paper 上的公式，\n",
        "# 但是實際上，它在 forward 的時候，卻把 attn_ln 放在了 attn 之前先做。\n",
        "# 這樣的話，就不是原來的公式了。\n",
        "# 由於 training 和 recog 必須一致，因此，我們不能改他們的 forward，\n",
        "# 只能將錯就錯，改他們的 model definition。\n",
        "# 他們的 forward 也許不算大錯，可能還是可以收斂，但畢竟不是原來的公式，\n",
        "# 寫在 model __init__，裡面的畢竟是原來的公式。因此我們把它在 __init__ 裡面改回來(反映真正程式的運作)。\n",
        "# 再仔細看一下，我們發現，其實都是 LayerNorm， 只差開頭和結尾的位置，可能少做或多做一次。\n",
        "# 而 LayerNorm 在最後一層重複做，或最前層少做，應該不會有太大的影響。\n",
        "#\n",
        "# 總之，發現原程式有 bug，也是功德一件，找個時間在與作者討論一下。\n",
        "#\n",
        "\n",
        "theMd_ORI= ''' Whisper(\n",
        "  (encoder): AudioEncoder(\n",
        "  \n",
        "    (conv1): Conv1d( 80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
        "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
        "\n",
        "    (blocks): ModuleList(\n",
        "      (0-3): 4 x ResidualAttentionBlock(\n",
        "        (attn): MultiHeadAttention(\n",
        "          (query):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (key):    Linear(in_features=384, out_features=384, bias=False)\n",
        "          (value):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (out):    Linear(in_features=384, out_features=384, bias=True)\n",
        "        )\n",
        "        (attn_ln):  LayerNorm((384,), eps=1e-05, elementwise_affine=True) ## BUG 在此\n",
        "\n",
        "        (mlp): Sequential(\n",
        "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
        "          (1): GELU(approximate='none')\n",
        "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
        "        )\n",
        "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) ## BUG 在此\n",
        "      )\n",
        "    )\n",
        "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        "\n",
        "  (decoder): TextDecoder(\n",
        "  \n",
        "    (token_embedding): Embedding(51_865, 384)\n",
        "\n",
        "    (blocks): ModuleList(\n",
        "      (0-3): 4 x ResidualAttentionBlock(\n",
        "        (attn): MultiHeadAttention(\n",
        "          (query):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (key):    Linear(in_features=384, out_features=384, bias=False)\n",
        "          (value):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (out):    Linear(in_features=384, out_features=384, bias=True)\n",
        "        )\n",
        "        (attn_ln):  LayerNorm((384,), eps=1e-05, elementwise_affine=True) ## BUG 在此\n",
        "\n",
        "        (cross_attn): MultiHeadAttention(\n",
        "          (query):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (key):    Linear(in_features=384, out_features=384, bias=False)\n",
        "          (value):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (out):    Linear(in_features=384, out_features=384, bias=True)\n",
        "        )\n",
        "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) ## BUG 在此\n",
        "\n",
        "        (mlp): Sequential(\n",
        "          (0): Linear(in_features=384, out_features=1_536, bias=True)\n",
        "          (1): GELU(approximate='none')\n",
        "          (2): Linear(in_features=1_536, out_features=384, bias=True)\n",
        "        )\n",
        "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) ## BUG 在此\n",
        "      )\n",
        "    )\n",
        "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        ")'''\n",
        "\n",
        "\n",
        "theModel_ryEdit= '''md= Whisper(\n",
        "\n",
        "  (encoder): AudioEncoder(\n",
        "  \n",
        "    (conv1): Conv1d( 80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
        "    (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
        "    \n",
        "    (blocks): ModuleList(\n",
        "      (0-3): 4 x ResidualAttentionBlock(\n",
        "        \n",
        "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) #### ryEdit\n",
        "        \n",
        "        (attn): MultiHeadAttention(\n",
        "          (query):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (key):    Linear(in_features=384, out_features=384, bias=False)\n",
        "          (value):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (out):    Linear(in_features=384, out_features=384, bias=True)\n",
        "        )\n",
        "        \n",
        "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) #### ryEdit\n",
        "        \n",
        "        (mlp): Sequential(\n",
        "          (0): Linear(in_features=384, out_features=1_536, bias=True)\n",
        "          (1): GELU(approximate='none')\n",
        "          (2): Linear(in_features=1_536, out_features=384, bias=True)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    \n",
        "    (ln_post): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        "\n",
        "  (decoder): TextDecoder(\n",
        "    \n",
        "    (token_embedding): Embedding(51_865, 384)\n",
        "    \n",
        "    (blocks): ModuleList(\n",
        "      (0-3): 4 x ResidualAttentionBlock(\n",
        "        \n",
        "        (attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) #### ryEdit\n",
        "        \n",
        "        (attn): MultiHeadAttention(\n",
        "          (query):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (key):    Linear(in_features=384, out_features=384, bias=False)\n",
        "          (value):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (out):    Linear(in_features=384, out_features=384, bias=True)\n",
        "        )\n",
        "        \n",
        "        (cross_attn_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) #### ryEdit\n",
        "        \n",
        "        (cross_attn): MultiHeadAttention(\n",
        "          (query):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (key):    Linear(in_features=384, out_features=384, bias=False)\n",
        "          (value):  Linear(in_features=384, out_features=384, bias=True)\n",
        "          (out):    Linear(in_features=384, out_features=384, bias=True)\n",
        "        )\n",
        "        \n",
        "        (mlp_ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True) #### ryEdit\n",
        "        \n",
        "        (mlp): Sequential(\n",
        "          (0): Linear(in_features=384, out_features=1_536, bias=True)\n",
        "          (1): GELU(approximate='none')\n",
        "          (2): Linear(in_features=1_536, out_features=384, bias=True)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    \n",
        "    (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        ")'''\n",
        "\n",
        "\n",
        "#%% %% dive into the model\n",
        "\n",
        "wav= get_wav() # torch.Size([580_160]) # 36.26 sec\n",
        "\n",
        "#%%\n",
        "a30sec=  wp.pad_or_trim(wav)              # -->torch.Size([480_000])\n",
        "mel= wp.audio.log_mel_spectrogram(a30sec) # -->torch.Size([80, 3000])\n",
        "mel= mel[None, ...]                       # -->torch.Size([1, 80, 3000])\n",
        "\n",
        "#%%\n",
        "Xa= enc= md.encoder(mel)                  # -->torch.Size([1, 1500, 384])\n",
        "#%% 給定開頭的文字 X，\n",
        "# 在上面聲音(特徵) Xa 壓陣的條件下，\n",
        "# 不斷預測接下來的文字、、\n",
        "\n",
        "X0= tok= torch.tensor(\n",
        "   [[50_258, 50_259, 50_359]]\n",
        "   ) # torch.Size([1, 3])\n",
        "X0= X0.to(device)\n",
        "\n",
        "X0_txt= txt4tok(X0)  \n",
        "print(f'{X0_txt= }')\n",
        "\n",
        "for i in range(100):\n",
        "  \n",
        "  X1= md.decoder(X0, Xa)               # torch.Size([1, 3, 51865])\n",
        "\n",
        "  x_next= X1[..., -1,:]                    # torch.Size([1, 51865])\n",
        "  y_next= x_next.argmax(dim=-1, keepdims=True) # torch.Size([1, 1])\n",
        "  X0=  torch.cat([X0, y_next], axis=-1)     # torch.Size([1, 4])\n",
        "  \n",
        "  y_next= txt4tok(y_next)           # '<|0.00|> Backward, bed, bird, ...'\n",
        "  print(f'{i= }, {y_next= }')\n",
        "#%%\n",
        "X0.shape # torch.Size([1, 103])\n",
        "\n",
        "X0_txt= txt4tok(X0)  \n",
        "print(f'{i= }, {X0_txt= }')\n",
        "\n",
        "# %%\n"
      ]
    }
  ]
}